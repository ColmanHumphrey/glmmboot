---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

# glmmboot

The goal of glmmboot is to provide a simple method to create
bootstrap confidence intervals using a wide set of models. For 
models with random effects, the default behaviour will 
be to block sample over the effect with the largest entropy 
(generally the one with the most levels); with no random effects,
performs case resampling. glmmboot should work for models that produce 
multiple sets of coefficients too.

The only requirements are that the model works with the 
function `update`, to change the data; and that the coefficients
are extractable using coef(summary(model)): either directly, or
stored in it as a list. This includes e.g. zero-inflated models, which
produce two matrices of coefficients.

It may be desired to run this package in parallel. The best way is
to use the `future` backend. You do that by specifying the backend
`future::plan` setup, and then setting `parallelism = "future"`
(although it actually calls `future.apply::future_lapply, so that should be
installed too for this).

While in some cases the data will be automatically extracted, you should
supply it manually.

## Installation

glmmboot is on CRAN, so you can install it normally:

```{r, eval = FALSE}
install.packages("glmmboot")
```

Or the development version:
```{r gh-installation, eval = FALSE}
devtools::install_github("ColmanHumphrey/glmmboot")
```

## Example

We'll provide a quick example using glm. First we'll set up some data:

```{r}
x1 <- rnorm(50)
x2 <- runif(50)

expit <- function(x){exp(x) / (1 + exp(x))}

y_mean <- expit(0.2 - 0.3 * x1 + 0.4 * x2)

y <- rbinom(50, 1, prob = y_mean)

sample_frame = data.frame(x1 = x1, x2 = x2, y = y)
```

Typically this model is fit with logistic regression:

```{r}
base_run <- glm(y ~ x1 + x2, family = binomial(link = 'logit'), data = sample_frame)

summary(base_run)
```

Let's run a bootstrap.

```{r}
library(glmmboot)
set.seed(15278086) # Happy for Nadia and Alan
boot_results <- bootstrap_model(base_model = base_run, 
                                base_data = sample_frame,
                                resamples = 999)
```

And the results:
```{r}
print(boot_results)
```

The estimates are the same, since we just pull from the base model. The intervals are 
similar to the base model, although slightly narrower: typical logistic regression is fractionally
conservative at `N = 50`.

An example with a zero-inflated model (from the `glmmTMB` docs):

```{r}
library(glmmTMB)

owls <- transform(Owls,
                  nest = reorder(Nest, NegPerChick),
                  ncalls = SiblingNegotiation,
                  ft = FoodTreatment)

fit_zipoisson <- glmmTMB(
    ncalls ~ (ft + ArrivalTime) * SexParent +
        offset(log(BroodSize)) + (1 | nest),
    data = owls,
    ziformula = ~1,
    family = poisson)

summary(fit_zipoisson)
```
Let's run the bootstrap (ignore the actual results, 3 resamples is basically meaningless - just for illustration):
```{r}
zi_results <- bootstrap_model(base_model = fit_zipoisson,
                              base_data = owls,
                              resamples = 3,
                              parallelism = "future")
print(zi_results)
```

We could also have run this with the `future.apply` backend:
```{r, eval = FALSE}
library(future.apply)
plan("multiprocess")

zi_results <- bootstrap_model(base_model = fit_zipoisson,
                              base_data = owls,
                              resamples = 1000,
                              parallelism = "future")
```
